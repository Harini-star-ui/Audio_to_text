import json
import queue
import sys
import vosk
import pyaudio
import wave
import datetime

def transcribe_live():
    model_path = "vosk_model"  # Change this path if your model is elsewhere
    model = vosk.Model(model_path)

    # Audio settings
    sample_rate = 16000
    buffer_size = 4000  # Buffer size for audio input

    # Initialize recognizer
    recognizer = vosk.KaldiRecognizer(model, sample_rate)

    # Set up PyAudio
    audio_queue = queue.Queue()

    def callback(in_data, frame_count, time_info, status):
        audio_queue.put(in_data)
        return (in_data, pyaudio.paContinue)

    audio = pyaudio.PyAudio()
    stream = audio.open(format=pyaudio.paInt16, channels=1,
                        rate=sample_rate, input=True,
                        frames_per_buffer=buffer_size,
                        stream_callback=callback)

    # Set up WAV file for saving audio
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    wav_filename = f"transcribed_audio_{timestamp}.wav"
    wav_file = wave.open(wav_filename, "wb")
    wav_file.setnchannels(1)
    wav_file.setsampwidth(audio.get_sample_size(pyaudio.paInt16))
    wav_file.setframerate(sample_rate)

    stream.start_stream()
    print(f"Listening... Speak into the microphone. Audio will be saved as {wav_filename}")

    try:
        while True:
            data = audio_queue.get()
            wav_file.writeframes(data)  # Save audio data to WAV file

            if recognizer.AcceptWaveform(data):
                result = json.loads(recognizer.Result())
                text = result.get("text", "")
                if text:
                    print("Recognized:", text)
    except KeyboardInterrupt:
        print("\nStopping transcription.")
    finally:
        # Stop and close everything properly
        stream.stop_stream()
        stream.close()
        audio.terminate()
        wav_file.close()
        print(f"Audio saved as {wav_filename}")

if __name__ == "__main__":
    transcribe_live()
