import json
import queue
import sys
import vosk
import pyaudio

def transcribe_live():
    model_path = "vosk_model"  # Change this path if your model is elsewhere
    model = vosk.Model(model_path)

    # Audio settings
    sample_rate = 16000
    buffer_size = 4000  # Buffer size for audio input

    # Initialize recognizer
    recognizer = vosk.KaldiRecognizer(model, sample_rate)

    # Set up PyAudio
    audio_queue = queue.Queue()
    def callback(in_data, frame_count, time_info, status):
        audio_queue.put(in_data)
        return (in_data, pyaudio.paContinue)

    audio = pyaudio.PyAudio()
    stream = audio.open(format=pyaudio.paInt16, channels=1,
                        rate=sample_rate, input=True,
                        frames_per_buffer=buffer_size,
                        stream_callback=callback)

    stream.start_stream()
    print("Listening... Speak into the microphone.")

    try:
        while True:
            data = audio_queue.get()
            if recognizer.AcceptWaveform(data):
                result = json.loads(recognizer.Result())
                text = result.get("text", "")
                if text:
                    print("Recognized:", text)
    except KeyboardInterrupt:
        print("\nStopping transcription.")
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

if __name__ == "__main__":
    transcribe_live()
